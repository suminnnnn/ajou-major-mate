import os
import tempfile
import re
from typing import List, Dict, Any
from uuid import uuid4
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageDocumentParseLoader
from app.domains.base_ingestor import BaseIngestor
from app.utils.s3_uploader import upload_file_to_s3
from bs4 import BeautifulSoup

class CurriculumIngestor(BaseIngestor):
    def ingest(self, data_path: str) -> List[Document]:
        docs: List[Document] = []
        
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        full_path = os.path.join(base_dir, data_path)
        
        chunk_index = 0

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            length_function=len
        )

        for filename in os.listdir(full_path):
            if not filename.lower().endswith(".pdf"):
                continue

            file_path = os.path.join(full_path, filename)
            department = os.path.splitext(filename)[0]
            print(f"ğŸ“„ Loading {filename} (department={department})")

            # UpstageDocumentParseLoader ì‚¬ìš©
            loader = UpstageDocumentParseLoader(
                file_path=file_path,
                split="page"  # í˜ì´ì§€ë³„ë¡œ ë¶„í• 
            )
            
            parsed_docs = loader.load()
            
            for page_num, doc in enumerate(parsed_docs):
                page_content = doc.page_content
                
                # ìˆœì°¨ì ìœ¼ë¡œ ë‚´ìš© ì²˜ë¦¬ (í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ ìˆœì„œëŒ€ë¡œ)
                sequential_data = self._process_content_sequentially(
                    page_content, department, filename, page_num, chunk_index, splitter
                )
                docs.extend(sequential_data["documents"])
                chunk_index = sequential_data["next_chunk_index"]

        return docs

    def _process_content_sequentially(self, html_content: str, department: str, filename: str, 
                                    page_num: int, start_chunk_index: int, splitter) -> dict:
        """HTML ë‚´ìš©ì„ ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ê³  í…Œì´ë¸” ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ ì €ì¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë””ë²„ê¹…ìš© í”Œë˜ê·¸
        debug_mode = (department == "êµ­ë°©ë””ì§€í„¸ìœµí•©í•™ê³¼")
        
        if debug_mode:
            print(f"\nğŸ” [DEBUG] Processing page {page_num} of {department}")
        
        # 1ë‹¨ê³„: ëª¨ë“  ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ê³  ë¶„ë¥˜
        elements = self._extract_elements_sequentially(soup, debug_mode)
        
        # 2ë‹¨ê³„: í…Œì´ë¸” ì£¼ë³€ í…ìŠ¤íŠ¸ ë³‘í•©
        merged_chunks = self._merge_table_with_context(elements, debug_mode)
        
        # 3ë‹¨ê³„: Document ê°ì²´ ìƒì„±
        documents = []
        chunk_index = start_chunk_index
        
        for chunk_data in merged_chunks:
            if chunk_data["type"] == "table_with_context":
                # í…Œì´ë¸” + ì»¨í…ìŠ¤íŠ¸ ì²­í¬
                combined_content = ""
                
                # ì´ì „ í…ìŠ¤íŠ¸ ì¶”ê°€
                if chunk_data["prev_text"]:
                    combined_content += f"[ì œëª©/ì„¤ëª…]\n{chunk_data['prev_text']}\n\n"
                
                # í…Œì´ë¸” ì¶”ê°€
                combined_content += f"[í…Œì´ë¸”]\n{chunk_data['table_content']}\n\n"
                
                # ë‹¤ìŒ í…ìŠ¤íŠ¸ ì¶”ê°€
                if chunk_data["next_text"]:
                    combined_content += f"[ë¶€ê°€ì„¤ëª…]\n{chunk_data['next_text']}"
                
                if debug_mode:
                    print(f"ğŸ“Š [DEBUG] Table chunk {chunk_index}:")
                    print(f"   - Prev text: {bool(chunk_data['prev_text'])}")
                    print(f"   - Table: âœ“")
                    print(f"   - Next text: {bool(chunk_data['next_text'])}")
                    print(f"   - Combined length: {len(combined_content)}")
                
                documents.append(Document(
                    page_content=combined_content.strip(),
                    metadata={
                        "type": "table_with_context",
                        "department": department,
                        "source_file": filename,
                        "chunk_index": chunk_index,
                        "page": page_num,
                        "table_index": chunk_data["table_index"],
                        "image_url": chunk_data["image_url"],
                        "has_prev_text": bool(chunk_data["prev_text"]),
                        "has_next_text": bool(chunk_data["next_text"])
                    }
                ))
                chunk_index += 1
                
            elif chunk_data["type"] == "text":
                # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í¬ (í…Œì´ë¸”ê³¼ ë³‘í•©ë˜ì§€ ì•Šì€ ê²ƒ)
                text_chunks = splitter.split_text(chunk_data["content"])
                
                for idx, chunk in enumerate(text_chunks):
                    if chunk.strip():
                        if debug_mode:
                            print(f"ğŸ“ [DEBUG] Text chunk {chunk_index}: {len(chunk)} chars")
                        
                        documents.append(Document(
                            page_content=chunk,
                            metadata={
                                "type": "text",
                                "department": department,
                                "source_file": filename,
                                "chunk_index": chunk_index,
                                "page": page_num,
                                "text_index": idx,
                                "element_tag": chunk_data.get("element_tag", "unknown")
                            }
                        ))
                        chunk_index += 1
        
        if debug_mode:
            print(f"âœ… [DEBUG] Page {page_num} processed: {len(documents)} chunks created")
        
        return {
            "documents": documents,
            "next_chunk_index": chunk_index
        }

    def _extract_elements_sequentially(self, soup: BeautifulSoup, debug_mode: bool) -> List[Dict[str, Any]]:
        """HTMLì—ì„œ ìš”ì†Œë“¤ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        elements = []
        
        for element in soup.find_all(['table', 'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            if element.name == 'table':
                markdown_table = self._convert_table_to_markdown(element)
                if markdown_table.strip():
                    elements.append({
                        "type": "table",
                        "content": markdown_table,
                        "element": element,
                        "original_index": len(elements)
                    })
                element.decompose()
            else:
                text_content = element.get_text(strip=True)
                if text_content and len(text_content) > 10:
                    elements.append({
                        "type": "text",
                        "content": text_content,
                        "element_tag": element.name,
                        "original_index": len(elements)
                    })
                element.decompose()
        
        # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        remaining_text = soup.get_text(separator='\n', strip=True)
        remaining_text = re.sub(r'\n\s*\n', '\n\n', remaining_text)
        if remaining_text.strip():
            elements.append({
                "type": "text",
                "content": remaining_text,
                "element_tag": "remaining",
                "original_index": len(elements)
            })
        
        if debug_mode:
            print(f"ğŸ”„ [DEBUG] Extracted {len(elements)} elements:")
            for i, elem in enumerate(elements):
                print(f"   {i}: {elem['type']} ({len(elem['content'])} chars)")
        
        return elements

    def _merge_table_with_context(self, elements: List[Dict[str, Any]], debug_mode: bool) -> List[Dict[str, Any]]:
        """í…Œì´ë¸”ê³¼ ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©"""
        merged_chunks = []
        used_indices = set()
        
        for i, element in enumerate(elements):
            if element["type"] == "table" and i not in used_indices:
                # ì´ì „ í…ìŠ¤íŠ¸ ì°¾ê¸°
                prev_text = ""
                prev_idx = i - 1
                if prev_idx >= 0 and prev_idx not in used_indices and elements[prev_idx]["type"] == "text":
                    prev_text = elements[prev_idx]["content"]
                    used_indices.add(prev_idx)
                
                # ë‹¤ìŒ í…ìŠ¤íŠ¸ ì°¾ê¸°
                next_text = ""
                next_idx = i + 1
                if next_idx < len(elements) and next_idx not in used_indices and elements[next_idx]["type"] == "text":
                    next_text = elements[next_idx]["content"]
                    used_indices.add(next_idx)
                
                # í…Œì´ë¸” ì´ë¯¸ì§€ ìƒì„±
                image_url = ""
                if "element" in element:
                    image_url = self._create_table_image(
                        element["element"], "", "", 0, len(merged_chunks)
                    )
                
                merged_chunks.append({
                    "type": "table_with_context",
                    "table_content": element["content"],
                    "prev_text": prev_text,
                    "next_text": next_text,
                    "table_index": len([c for c in merged_chunks if c["type"] == "table_with_context"]),
                    "image_url": image_url
                })
                
                used_indices.add(i)
                
                if debug_mode:
                    print(f"ğŸ”— [DEBUG] Merged table {i}:")
                    print(f"   - Prev text ({prev_idx}): {len(prev_text)} chars")
                    print(f"   - Table: {len(element['content'])} chars")
                    print(f"   - Next text ({next_idx}): {len(next_text)} chars")
        
        # ì‚¬ìš©ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ìš”ì†Œë“¤ ì¶”ê°€
        for i, element in enumerate(elements):
            if i not in used_indices and element["type"] == "text":
                merged_chunks.append({
                    "type": "text",
                    "content": element["content"],
                    "element_tag": element.get("element_tag", "unknown")
                })
                
                if debug_mode:
                    print(f"ğŸ“„ [DEBUG] Standalone text {i}: {len(element['content'])} chars")
        
        return merged_chunks

    def _convert_table_to_markdown(self, table) -> str:
        """HTML í…Œì´ë¸”ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        rows = []
        
        # í—¤ë” ì²˜ë¦¬
        thead = table.find('thead')
        if thead:
            header_rows = thead.find_all('tr')
            for row in header_rows:
                cells = row.find_all(['th', 'td'])
                row_data = []
                for cell in cells:
                    colspan = int(cell.get('colspan', 1))
                    rowspan = int(cell.get('rowspan', 1))
                    cell_text = cell.get_text(strip=True)
                    
                    # colspan ì²˜ë¦¬
                    row_data.append(cell_text)
                    for _ in range(colspan - 1):
                        row_data.append("")
                
                rows.append("| " + " | ".join(row_data) + " |")
            
            # í—¤ë” êµ¬ë¶„ì„  ì¶”ê°€
            if rows:
                separator = "| " + " | ".join(["---"] * len(row_data)) + " |"
                rows.append(separator)
        
        # ë°”ë”” ì²˜ë¦¬
        tbody = table.find('tbody')
        if tbody:
            body_rows = tbody.find_all('tr')
        else:
            # tbodyê°€ ì—†ëŠ” ê²½ìš° tableì—ì„œ ì§ì ‘ tr ì°¾ê¸°
            body_rows = table.find_all('tr')
            if thead:  # í—¤ë”ê°€ ìˆì—ˆë‹¤ë©´ í—¤ë” í–‰ë“¤ì€ ì œì™¸
                header_row_count = len(thead.find_all('tr'))
                body_rows = body_rows[header_row_count:]
        
        for row in body_rows:
            cells = row.find_all(['td', 'th'])
            row_data = []
            for cell in cells:
                colspan = int(cell.get('colspan', 1))
                cell_text = cell.get_text(strip=True)
                
                row_data.append(cell_text)
                for _ in range(colspan - 1):
                    row_data.append("")
            
            if row_data:  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                rows.append("| " + " | ".join(row_data) + " |")
        
        return "\n".join(rows)

    def _create_table_image(self, table, department: str, filename: str, page_num: int, tbl_idx: int) -> str:
        """í…Œì´ë¸” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  S3ì— ì—…ë¡œë“œ (ì˜µì…˜ - í•„ìš”ì‹œ êµ¬í˜„)"""
        # í˜„ì¬ëŠ” ë¹ˆ URL ë°˜í™˜, í•„ìš”ì‹œ ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ë¡œì§ êµ¬í˜„
        # ì˜ˆ: HTML to Image ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ì´ë¯¸ì§€ ìƒì„±
        try:
            # ê°„ë‹¨í•œ êµ¬í˜„ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì´ë¯¸ì§€ ìƒì„± í•„ìš”)
            s3_key = f"{department}/{filename}_p{page_num}_t{tbl_idx}_{uuid4().hex}.png"
            
            # ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ë¡œì§ì€ ìƒëµí•˜ê³  placeholder URL ë°˜í™˜
            # í•„ìš”ì‹œ wkhtmltopdf, playwright, selenium ë“±ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„
            image_url = f"https://placeholder-url/{s3_key}"
            
            return image_url
        except Exception as e:
            print(f"âš ï¸ Failed to create table image: {e}")
            return ""